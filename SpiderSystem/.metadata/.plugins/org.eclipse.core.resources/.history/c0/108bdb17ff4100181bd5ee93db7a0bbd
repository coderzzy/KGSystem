package test;

import com.github.xjtushilei.core.Spider;

public class TestSpider {
	
	//爬取《交大新闻网》的所有信息，并将信息输出到文本文件！
	
	public static void main(String[] args){
		Spider.build()
        .setProcessor(myPageProcessor)
        .setSaver(mySaver)
        .addUrlSeed("http://news.xjtu.edu.cn")
        .addRegexRule("http://news.xjtu.edu.cn/.*htm") //只爬取新闻类的页面
        .run();
	}
	
	
	/**
     * 实现自己逻辑的页面解析功能！
     * <p>
     * 这里是在一个文件里实现的，若果你的功能比较多，完全可以用新的class文件来生成，并在上面set即可！
     */
    static PageProcessor myPageProcessor = new PageProcessor() {

        @Override
        public Page process(Page page) {

            //如果不匹配，则不进行解析！
            if (!Pattern.matches("http://news.xjtu.edu.cn/info/.*htm", page.getUrlSeed().getUrl())) {
                return page;
            }

            Document htmldoc = page.getDocument();
            //select返回的是一个数组，所以需要first，相关语法请google“jsoup select语法”和“cssquery”
            try {
                String title = htmldoc.select(".d_title").first().text();
                String text = htmldoc.select(".d_detail").first().text();

                //用来存放爬取到的信息，供之后存储！map类型的即可，可以自定义各种嵌套！
                Map<String, String> items = new HashMap<String, String>();
                items.put("title", title);
                items.put("text", text);
                items.put("url", page.getUrlSeed().getUrl());

                //放入items中，之后会自动保存（如果你自己实现了下载器，请自己操作它。如下我自定义了自己的下载器，并将它保存到了文本中！）！
                page.setItems(items);
            } catch (NullPointerException e) {
                System.out.println("没有解析到相关东西！跳过");
            }


            return page;
        }
        /**
         * 推荐在这里做优先级处理的东西。或者您可以做任何其他的事情。关于优先级的使用，我们在接下来的“优先级Spider”中会讲解
         */
        @Override
        public Page processNewUrlSeeds(Page page) {
            return page;
        }
    };
    
    
}
